{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(data_name):\n",
    "    if(data_name == 'mnist'):\n",
    "        model = Net_mnist()\n",
    "    elif(data_name == 'cifar10'):\n",
    "        model = Net_cifar10()\n",
    "    elif(data_name == 'purchase'):\n",
    "        model = Net_purchase()\n",
    "    elif(data_name == 'adult'):\n",
    "        model = Net_adult()\n",
    "    return model\n",
    "\n",
    "\n",
    "class Net_mnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_mnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Net_purchase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_purchase, self).__init__()\n",
    "        self.fc1 = nn.Linear(600, 300)\n",
    "        self.fc2 = nn.Linear(300, 50)\n",
    "        self.fc3 = nn.Linear(50,2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x   \n",
    "    \n",
    "\n",
    "class Net_adult(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_adult, self).__init__()\n",
    "        self.fc1 = nn.Linear(108, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10,2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x     \n",
    "\n",
    "\n",
    "\n",
    "class Net_cifar10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_cifar10, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_init(FL_params):\n",
    "    \n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True} if FL_params.cuda_state else {}\n",
    "    trainset, testset = data_set(FL_params.data_name)\n",
    "    # shadow_split_idx = [int(whole_trainset.__len__()/2), int(whole_trainset.__len__()) -int(whole_trainset.__len__()/2)]\n",
    "    # trainset, shadow_trainset = torch.utils.data.random_split(whole_trainset, shadow_split_idx)\n",
    "    \n",
    "    # shadow_split_idx = [int(whole_testset.__len__()/2), int(whole_testset.__len__()) -int(whole_testset.__len__()/2)]\n",
    "    # testset, shadow_testset = torch.utils.data.random_split(whole_testset, shadow_split_idx)\n",
    "    # \n",
    "    \n",
    "    # train_loader = DataLoader(trainset, batch_size=FL_params.local_batch_size, shuffle=True, **kwargs)\n",
    "    #构建测试数据加载器\n",
    "    test_loader = DataLoader(testset, batch_size=FL_params.test_batch_size, shuffle=True, **kwargs)\n",
    "    # shadow_test_loader = DataLoader(shadow_testset, batch_size=FL_params.test_batch_size, shuffle=False, **kwargs)\n",
    "                \n",
    "    \n",
    "    #将数据按照训练的trainset，均匀的分配成N-client份，所有分割得到dataset都保存在一个list中\n",
    "    split_index = [int(trainset.__len__()/FL_params.N_total_client)]*(FL_params.N_total_client-1)\n",
    "    split_index.append(int(trainset.__len__() - int(trainset.__len__()/FL_params.N_total_client)*(FL_params.N_total_client-1)))\n",
    "    client_dataset = torch.utils.data.random_split(trainset, split_index)\n",
    "    \n",
    "    # split_index = [int(shadow_trainset.__len__()/FL_params.N_total_client)]*(FL_params.N_total_client-1)\n",
    "    # split_index.append(int(shadow_trainset.__len__() - int(shadow_trainset.__len__()/FL_params.N_total_client)*(FL_params.N_total_client-1)))\n",
    "    # shadow_client_dataset = torch.utils.data.random_split(shadow_trainset, split_index)\n",
    "    #将全局模型复制N-client次，然后构建每一个client模型的优化器，参数记录   \n",
    "    client_loaders = []\n",
    "    # shadow_client_sloaders = []\n",
    "    for ii in range(FL_params.N_total_client):\n",
    "        client_loaders.append(DataLoader(client_dataset[ii], FL_params.local_batch_size, shuffle=True, **kwargs))\n",
    "        # shadow_client_loaders.append(DataLoader(shadow_client_dataset[ii], FL_params.local_batch_size, shuffle=False, **kwargs))\n",
    "        '''\n",
    "        By now，我们已经将client用户的本地数据区分完成，存放在client_loaders中。每一个都对应的是某一个用户的私有数据\n",
    "        '''\n",
    "    \n",
    "    return client_loaders, test_loader\n",
    "\n",
    "\n",
    "def data_init_with_shadow(FL_params):\n",
    "    \n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True} if FL_params.cuda_state else {}\n",
    "    whole_trainset, whole_testset = data_set(FL_params.data_name)\n",
    "    shadow_split_idx = [int(whole_trainset.__len__()/2), int(whole_trainset.__len__()) -int(whole_trainset.__len__()/2)]\n",
    "    trainset, shadow_trainset = torch.utils.data.random_split(whole_trainset, shadow_split_idx)\n",
    "    \n",
    "    shadow_split_idx = [int(whole_testset.__len__()/2), int(whole_testset.__len__()) -int(whole_testset.__len__()/2)]\n",
    "    testset, shadow_testset = torch.utils.data.random_split(whole_testset, shadow_split_idx)\n",
    "    \n",
    "    \n",
    "    # train_loader = DataLoader(trainset, batch_size=FL_params.local_batch_size, shuffle=True, **kwargs)\n",
    "    #构建测试数据加载器\n",
    "    test_loader = DataLoader(testset, batch_size=FL_params.test_batch_size, shuffle=False, **kwargs)\n",
    "    shadow_test_loader = DataLoader(shadow_testset, batch_size=FL_params.test_batch_size, shuffle=False, **kwargs)\n",
    "                \n",
    "    \n",
    "    #将数据按照训练的trainset，均匀的分配成N-client份，所有分割得到dataset都保存在一个list中\n",
    "    split_index = [int(trainset.__len__()/FL_params.N_client)]*(FL_params.N_client-1)\n",
    "    split_index.append(int(trainset.__len__() - int(trainset.__len__()/FL_params.N_client)*(FL_params.N_client-1)))\n",
    "    client_dataset = torch.utils.data.random_split(trainset, split_index)\n",
    "    \n",
    "    split_index = [int(shadow_trainset.__len__()/FL_params.N_client)]*(FL_params.N_client-1)\n",
    "    split_index.append(int(shadow_trainset.__len__() - int(shadow_trainset.__len__()/FL_params.N_client)*(FL_params.N_client-1)))\n",
    "    shadow_client_dataset = torch.utils.data.random_split(shadow_trainset, split_index)\n",
    "    #将全局模型复制N-client次，然后构建每一个client模型的优化器，参数记录   \n",
    "    client_loaders = []\n",
    "    shadow_client_loaders = []\n",
    "    for ii in range(FL_params.N_client):\n",
    "        client_loaders.append(DataLoader(client_dataset[ii], FL_params.local_batch_size, shuffle=False, **kwargs))\n",
    "        shadow_client_loaders.append(DataLoader(shadow_client_dataset[ii], FL_params.local_batch_size, shuffle=False, **kwargs))\n",
    "        '''\n",
    "        By now，我们已经将client用户的本地数据区分完成，存放在client_loaders中。每一个都对应的是某一个用户的私有数据\n",
    "        '''\n",
    "    \n",
    "    return client_loaders, test_loader, shadow_client_loaders, shadow_test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_set(data_name):\n",
    "    if not data_name in ['mnist','purchase','adult','cifar10']:\n",
    "        raise TypeError('data_name should be a string, including mnist,purchase,adult,cifar10. ')\n",
    "    \n",
    "    #model: 2 conv. layers followed by 2 FC layers\n",
    "    if(data_name == 'mnist'):\n",
    "        trainset = datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "        testset = datasets.MNIST('./data', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "        \n",
    "    #model: ResNet-50\n",
    "    elif(data_name == 'cifar10'):\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        \n",
    "        trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                                download=True, transform=transform)\n",
    "        \n",
    "        testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                                download=True, transform=transform)\n",
    "    \n",
    "    #model: 2 FC layers\n",
    "    elif(data_name == 'purchase'):\n",
    "        xx = np.load(\"./data/purchase/purchase_xx.npy\")\n",
    "        yy = np.load(\"./data/purchase/purchase_y2.npy\")\n",
    "        # yy = yy.reshape(-1,1)\n",
    "        # enc = preprocessing.OneHotEncoder(categories='auto')\n",
    "        # enc.fit(yy)\n",
    "        # yy = enc.transform(yy).toarray()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(xx, yy, test_size=0.2, random_state=42)\n",
    "        \n",
    "        X_train_tensor = torch.Tensor(X_train).type(torch.FloatTensor)\n",
    "        X_test_tensor = torch.Tensor(X_test).type(torch.FloatTensor)\n",
    "        y_train_tensor = torch.Tensor(y_train).type(torch.LongTensor)\n",
    "        y_test_tensor = torch.Tensor(y_test).type(torch.LongTensor)\n",
    "        \n",
    "        trainset = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "        testset = TensorDataset(X_test_tensor,y_test_tensor)\n",
    "        \n",
    "    \n",
    "    #model: 2 FC layers\n",
    "    elif(data_name == 'adult'):\n",
    "        #load data\n",
    "        file_path = \"./data/adult/\"\n",
    "        data1 = pd.read_csv(file_path + 'adult.data', header=None)\n",
    "        data2 = pd.read_csv(file_path + 'adult.test', header=None)\n",
    "        data2 = data2.replace(' <=50K.', ' <=50K')    \n",
    "        data2 = data2.replace(' >50K.', ' >50K')\n",
    "        train_num = data1.shape[0]\n",
    "        data = pd.concat([data1,data2])\n",
    "       \n",
    "        #data transform: str->int\n",
    "        data = np.array(data, dtype=str)\n",
    "        labels = data[:,14]\n",
    "        le= LabelEncoder()\n",
    "        le.fit(labels)\n",
    "        labels = le.transform(labels)\n",
    "        data = data[:,:-1]\n",
    "        \n",
    "        categorical_features = [1,3,5,6,7,8,9,13]\n",
    "        # categorical_names = {}\n",
    "        for feature in categorical_features:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(data[:, feature])\n",
    "            data[:, feature] = le.transform(data[:, feature])\n",
    "            # categorical_names[feature] = le.classes_\n",
    "        data = data.astype(float)\n",
    "        \n",
    "        n_features = data.shape[1]\n",
    "        numerical_features = list(set(range(n_features)).difference(set(categorical_features)))\n",
    "        for feature in numerical_features:\n",
    "            scaler = MinMaxScaler()\n",
    "            sacled_data = scaler.fit_transform(data[:,feature].reshape(-1,1))\n",
    "            data[:,feature] = sacled_data.reshape(-1)\n",
    "        \n",
    "        #OneHotLabel\n",
    "        oh_encoder = ColumnTransformer(\n",
    "            [('oh_enc', OneHotEncoder(sparse=False), categorical_features),], \n",
    "            remainder='passthrough' )\n",
    "        oh_data = oh_encoder.fit_transform(data)\n",
    "        \n",
    "        xx = oh_data\n",
    "        yy = labels\n",
    "        #最终处理，xx进行规范化\n",
    "        xx = preprocessing.scale(xx)\n",
    "        yy = np.array(yy)\n",
    "        \n",
    "        xx = torch.Tensor(xx).type(torch.FloatTensor)\n",
    "        yy = torch.Tensor(yy).type(torch.LongTensor)\n",
    "        xx_train = xx[0:data1.shape[0],:]\n",
    "        xx_test = xx[data1.shape[0]:,:]\n",
    "        yy_train = yy[0:data1.shape[0]]\n",
    "        yy_test = yy[data1.shape[0]:]\n",
    "        \n",
    "        # trainset = Array2Dataset(xx_train, yy_train)\n",
    "        # testset = Array2Dataset(xx_test, yy_test)\n",
    "        trainset = TensorDataset(xx_train,yy_train)\n",
    "        testset = TensorDataset(xx_test,yy_test)\n",
    "        \n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_train_once(global_model, client_data_loaders, test_loader, FL_params):\n",
    "    #使用每个client的模型、优化器、数据，以client_models为训练初始模型，使用client用户本地的数据和优化器，更新得到upodate——client_models\n",
    "    #Note：需要注意的一点是，global_train_once只是在全局上对模型的参数进行一次更新\n",
    "    #Using the model, optimizer, and data of each client, training the initial model with client_models, updating the UPODate -- client_models using the client user's local data and optimizer\n",
    "    #Note: It is important to Note that global_train_once is only a global update to the parameters of the model\n",
    "    # update_client_models = list()\n",
    "    device = torch.device(\"cuda\" if FL_params.use_gpu*FL_params.cuda_state else \"cpu\")\n",
    "    device_cpu = torch.device(\"cpu\")\n",
    "    \n",
    "        \n",
    "    client_models = []\n",
    "    client_sgds = []\n",
    "    for ii in range(FL_params.N_client):\n",
    "        client_models.append(copy.deepcopy(global_model))\n",
    "        client_sgds.append(optim.SGD(client_models[ii].parameters(), lr=FL_params.local_lr, momentum=0.9))\n",
    "\n",
    "    \n",
    "    \n",
    "    for client_idx in range(FL_params.N_client):\n",
    "        if(((FL_params.if_retrain) and (FL_params.forget_client_idx == client_idx)) or ((FL_params.if_unlearning) and (FL_params.forget_client_idx == client_idx))):\n",
    "            \n",
    "            continue\n",
    "        # if((FL_params.if_unlearning) and (FL_params.forget_client_idx == client_idx)):\n",
    "        #     continue\n",
    "        # print(30*'-')\n",
    "        # print(\"Now training Client No.{}  \".format(client_idx))\n",
    "        model = client_models[client_idx]\n",
    "        optimizer = client_sgds[client_idx]\n",
    "        \n",
    "        \n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        \n",
    "        #local training\n",
    "        for local_epoch in range(FL_params.local_epoch):\n",
    "            for batch_idx, (data, target) in enumerate(client_data_loaders[client_idx]):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                pred = model(data)\n",
    "                criteria = nn.CrossEntropyLoss()\n",
    "                loss = criteria(pred, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            if(FL_params.train_with_test):\n",
    "                print(\"Local Client No. {}, Local Epoch: {}\".format(client_idx, local_epoch))\n",
    "                test(model, test_loader)\n",
    "        \n",
    "        \n",
    "        # if(FL_params.use_gpu*FL_params.cuda_state):\n",
    "        model.to(device_cpu)\n",
    "        client_models[client_idx] = model\n",
    "        \n",
    "    if(((FL_params.if_retrain) and (FL_params.forget_client_idx == client_idx))):\n",
    "        #只有retrian 需要丢弃client 模型；如果不是在retrain的话，就不需要丢弃模型\n",
    "        #Only retrian needs to discard the Client model;If it's not in Retrain, there's no need to discard the model\n",
    "        client_models.pop(FL_params.forget_client_idx)\n",
    "        return client_models\n",
    "    elif((FL_params.if_unlearning) and (FL_params.forget_client_idx in range(FL_params.N_client))):\n",
    "        client_models.pop(FL_params.forget_client_idx)\n",
    "        return client_models\n",
    "    else:\n",
    "        return client_models\n",
    "    \n",
    "    \n",
    "def fedavg(local_models):\n",
    "# def fedavg(local_models, local_model_weights=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    local_models : list of local models\n",
    "        DESCRIPTION.In federated learning, with the global_model as the initial model, each user uses a collection of local models updated with their local data.\n",
    "    local_model_weights : tensor or array\n",
    "        DESCRIPTION. The weight of each local model is usually related to the accuracy rate and number of data of the local model.(Bypass)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    update_global_model\n",
    "        Updated global model using fedavg algorithm\n",
    "    \"\"\"\n",
    "    # N = len(local_models)\n",
    "    # new_global_model = copy.deepcopy(local_models[0])\n",
    "    # print(len(local_models))\n",
    "    global_model = copy.deepcopy(local_models[0])\n",
    "    avg_state_dict = global_model.state_dict()\n",
    "    \n",
    "    local_state_dicts = list()\n",
    "    for model in local_models:\n",
    "        local_state_dicts.append(model.state_dict())\n",
    "    \n",
    "    \n",
    "    for layer in avg_state_dict.keys():\n",
    "        avg_state_dict[layer] *= 0 \n",
    "        for client_idx in range(len(local_models)):\n",
    "            avg_state_dict[layer] += local_state_dicts[client_idx][layer]\n",
    "        avg_state_dict[layer] /= len(local_models)\n",
    "    \n",
    "    global_model.load_state_dict(avg_state_dict)\n",
    "    return global_model \n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            criteria = nn.CrossEntropyLoss()\n",
    "            test_loss += criteria(output, target) # sum up batch loss\n",
    "            \n",
    "            pred = torch.argmax(output,axis=1)\n",
    "            test_acc += accuracy_score(pred,target)\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = test_acc/np.ceil(len(test_loader.dataset)/test_loader.batch_size)\n",
    "    print('Test set: Average loss: {:.8f}'.format(test_loss))         \n",
    "    print('Test set: Average acc:  {:.4f}'.format(test_acc))    \n",
    "    return (test_loss, test_acc)\n",
    "\n",
    "\n",
    "\n",
    "def FL_Train(init_global_model, client_data_loaders, test_loader, FL_params):\n",
    "    if(FL_params.if_retrain == True):\n",
    "        raise ValueError('FL_params.if_retrain should be set to False, if you want to train, not retrain FL model')\n",
    "    if(FL_params.if_unlearning == True):\n",
    "        raise ValueError('FL_params.if_unlearning should be set to False, if you want to train, not unlearning FL model')\n",
    "    \n",
    "    # if(FL_params._save_all_models == False):\n",
    "    #     # print(\"FL Training without Forgetting...\")\n",
    "    #     global_model = init_global_model\n",
    "    #     for epoch in range(FL_params.global_epoch):\n",
    "    #         client_models = global_train_once(global_model, client_data_loaders, test_loader, FL_params)\n",
    "    #         #IMPORTANT：这里有一点要注意，就是global_train_once在训练过程中，是直接在input的client_models上进行训练，因此output的client_models与input的client_models是同一组模型，只不过input没有经过训练，而output经过了训练。\n",
    "    # #   IMPORTANT：因此，为了实现Federated unlearning，我们需要在global train之前就将client——models中的模型进行保存。可以使用deepcopy，或者硬盘io方式。\n",
    "    #         global_model = fedavg(client_models)\n",
    "    #         print(30*'^')\n",
    "    #         print(\"Global training epoch = {}\".format(epoch))\n",
    "    #         # test(global_model, test_loader)\n",
    "    #         print(30*'v')\n",
    "        \n",
    "    #     return global_model\n",
    "    # elif (FL_params._save_all_models == True):\n",
    "        # print(\"FL Training with Forgetting...\")\n",
    "    all_global_models = list()\n",
    "    all_client_models = list()\n",
    "    global_model = init_global_model\n",
    "    \n",
    "    all_global_models.append(copy.deepcopy(global_model))\n",
    "    \n",
    "    for epoch in range(FL_params.global_epoch):\n",
    "        client_models = global_train_once(global_model, client_data_loaders, test_loader, FL_params)\n",
    "        #IMPORTANT：这里有一点要注意，就是global_train_once在训练过程中，是直接在input的client_models上进行训练，因此output的client_models与input的client_models是同一组模型，只不过input没有经过训练，而output经过了训练。\n",
    "   #IMPORTANT：因此，为了实现Federated unlearning，我们需要在global train之前就将client——models中的模型进行保存。可以使用deepcopy，或者硬盘io方式。\n",
    "    #IMPORTANT: It is IMPORTANT to note here that global_train_once is trained directly on the input client_models during training, so the output's client_models are the same set of models as the input's client_models, except that the input is untrained while the output is trained.\n",
    "    #Therefore, in order to implement Federated Unlearning, we need to save the models in Client -- Models before global Train.You can use DeepCopy, or hard disk IO.\n",
    "        all_client_models += client_models\n",
    "        global_model = fedavg(client_models)\n",
    "        # print(30*'^')\n",
    "        print(\"Global Federated Learning epoch = {}\".format(epoch))\n",
    "        # test(global_model, test_loader)\n",
    "        # print(30*'v')\n",
    "        # print(len(all_client_models))\n",
    "        all_global_models.append(copy.deepcopy(global_model))\n",
    "        \n",
    "    return all_global_models, all_client_models\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def FL_Retrain(init_global_model, client_data_loaders, test_loader, FL_params):\n",
    "    if(FL_params.if_retrain == False):\n",
    "        raise ValueError('FL_params.if_retrain should be set to True, if you want to retrain FL model')\n",
    "    if(FL_params.forget_client_idx not in range(FL_params.N_client)):\n",
    "        raise ValueError('FL_params.forget_client_idx should be in [{}], if you want to use standard FL train with forget the certain client dataset.'.format(range(FL_params.N_client)))\n",
    "    # forget_idx= FL_params.forget_idx\n",
    "    print('\\n')\n",
    "    print(5*\"#\"+\"  Federated Retraining Start  \"+5*\"#\")\n",
    "    # std_time = time.time()\n",
    "    print(\"Federated Retrain with Forget Client NO.{}\".format(FL_params.forget_client_idx))\n",
    "    retrain_GMs = list()\n",
    "    all_client_models = list()\n",
    "    retrain_GMs.append(copy.deepcopy(init_global_model))\n",
    "    global_model = init_global_model\n",
    "    for epoch in range(FL_params.global_epoch):\n",
    "        client_models = global_train_once(global_model, client_data_loaders, test_loader, FL_params)\n",
    "        #IMPORTANT：这里有一点要注意，就是global_train_once在训练过程中，是直接在input的client_models上进行训练，因此output的client_models与input的client_models是同一组模型，只不过input没有经过训练，而output经过了训练。\n",
    "        #IMPORTANT：这里有一点要注意，就是global_train_once在训练过程中，是直接在input的client_models上进行训练，因此output的client_models与input的client_models是同一组模型，只不过input没有经过训练，而output经过了训练。：It is important to note that global_train_once is trained directly on the input client_models during training, so the output's client_models are the same set of models as the input's client_models, except that the input is untrained while the output is trained.\n",
    "#   IMPORTANT：因此，为了实现Federated unlearning，我们需要在global train之前就将client——models中的模型进行保存。可以使用deepcopy，或者硬盘io方式。\n",
    "#IMPORTANT: Therefore, in order to implement Federated Unlearning, we need to save the models in Client -- Models before global Train.You can use DeepCopy, or hard disk IO.\n",
    "        global_model = fedavg(client_models)\n",
    "        # print(30*'^')\n",
    "        print(\"Global Retraining epoch = {}\".format(epoch))\n",
    "        # test(global_model, test_loader)\n",
    "        # print(30*'v')\n",
    "        retrain_GMs.append(copy.deepcopy(global_model))\n",
    "        \n",
    "        all_client_models += client_models\n",
    "    # end_time = time.time()\n",
    "    print(5*\"#\"+\"  Federated Retraining End  \"+5*\"#\")\n",
    "    return retrain_GMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unlearning_step_once(old_client_models, new_client_models, global_model_before_forget, global_model_after_forget):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    old_client_models : list of DNN models\n",
    "        When there is no choice to forget (if_forget=False), use the normal continuous learning training to get each user's local model.The old_client_models do not contain models of users that are forgotten.\n",
    "        Models that require forgotten users are not discarded in the Forget function\n",
    "    ref_client_models : list of DNN models\n",
    "        When choosing to forget (if_forget=True), train with the same Settings as before, except that the local epoch needs to be reduced, other parameters are set in the same way.\n",
    "        Using the above training Settings, the new global model is taken as the starting point and the reference model is trained.The function of the reference model is to identify the direction of model parameter iteration starting from the new global model\n",
    "        \n",
    "    global_model_before_forget : The old global model\n",
    "        DESCRIPTION.\n",
    "    global_model_after_forget : The New global model\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    return_global_model : After one iteration, the new global model under the forgetting setting\n",
    "\n",
    "    \"\"\"\n",
    "    old_param_update = dict()#Model Params： oldCM - oldGM_t\n",
    "    new_param_update = dict()#Model Params： newCM - newGM_t\n",
    "    \n",
    "    new_global_model_state = global_model_after_forget.state_dict()#newGM_t\n",
    "    \n",
    "    return_model_state = dict()#newGM_t + ||oldCM - oldGM_t||*(newCM - newGM_t)/||newCM - newGM_t||\n",
    "    \n",
    "    assert len(old_client_models) == len(new_client_models)\n",
    "    \n",
    "    for layer in global_model_before_forget.state_dict().keys():\n",
    "        old_param_update[layer] = 0*global_model_before_forget.state_dict()[layer]\n",
    "        new_param_update[layer] = 0*global_model_before_forget.state_dict()[layer]\n",
    "        \n",
    "        return_model_state[layer] = 0*global_model_before_forget.state_dict()[layer]\n",
    "        \n",
    "        for ii in range(len(new_client_models)):\n",
    "            old_param_update[layer] += old_client_models[ii].state_dict()[layer]\n",
    "            new_param_update[layer] += new_client_models[ii].state_dict()[layer]\n",
    "        old_param_update[layer] /= (ii+1)#Model Params： oldCM\n",
    "        new_param_update[layer] /= (ii+1)#Model Params： newCM\n",
    "        \n",
    "        old_param_update[layer] = old_param_update[layer] - global_model_before_forget.state_dict()[layer]#参数： oldCM - oldGM_t\n",
    "        new_param_update[layer] = new_param_update[layer] - global_model_after_forget.state_dict()[layer]#参数： newCM - newGM_t\n",
    "        \n",
    "        step_length = torch.norm(old_param_update[layer])#||oldCM - oldGM_t||\n",
    "        step_direction = new_param_update[layer]/torch.norm(new_param_update[layer])#(newCM - newGM_t)/||newCM - newGM_t||\n",
    "        \n",
    "        return_model_state[layer] = new_global_model_state[layer] + step_length*step_direction\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return_global_model = copy.deepcopy(global_model_after_forget)\n",
    "    \n",
    "    return_global_model.load_state_dict(return_model_state)\n",
    "    \n",
    "    return return_global_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def unlearning(old_GMs, old_CMs, client_data_loaders, test_loader, FL_params):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    old_global_models : list of DNN models\n",
    "        In standard federated learning, all the global models from each round of training are saved.\n",
    "    old_client_models : list of local client models\n",
    "        In standard federated learning, the server collects all user models after each round of training.\n",
    "    client_data_loaders : list of torch.utils.data.DataLoader\n",
    "        This can be interpreted as each client user's own data, and each Dataloader corresponds to each user's data\n",
    "    test_loader : torch.utils.data.DataLoader\n",
    "        The loader for the test set used for testing\n",
    "    FL_params : Argment（）\n",
    "        The parameter class used to set training parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    forget_global_model : One DNN model that has the same structure but different parameters with global_moedel\n",
    "        DESCRIPTION.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if(FL_params.if_unlearning == False):\n",
    "        raise ValueError('FL_params.if_unlearning should be set to True, if you want to unlearning with a certain user')\n",
    "        \n",
    "    if(not(FL_params.forget_client_idx in range(FL_params.N_client))):\n",
    "        raise ValueError('FL_params.forget_client_idx is note assined correctly, forget_client_idx should in {}'.format(range(FL_params.N_client)))\n",
    "    if(FL_params.unlearn_interval == 0 or FL_params.unlearn_interval >FL_params.global_epoch):\n",
    "        raise ValueError('FL_params.unlearn_interval should not be 0, or larger than the number of FL_params.global_epoch')\n",
    "    \n",
    "    old_global_models = copy.deepcopy(old_GMs)\n",
    "    old_client_models = copy.deepcopy(old_CMs)\n",
    "    \n",
    "    \n",
    "    forget_client = FL_params.forget_client_idx\n",
    "    for ii in range(FL_params.global_epoch):\n",
    "        temp = old_client_models[ii*FL_params.N_client : ii*FL_params.N_client+FL_params.N_client]\n",
    "        temp.pop(forget_client)#During Unlearn, the model saved by the forgotten user pops up\n",
    "        old_client_models.append(temp)\n",
    "    old_client_models = old_client_models[-FL_params.global_epoch:]\n",
    "        \n",
    "    \n",
    "    \n",
    "    GM_intv = np.arange(0,FL_params.global_epoch+1, FL_params.unlearn_interval, dtype=np.int16())\n",
    "    CM_intv  = GM_intv -1\n",
    "    CM_intv = CM_intv[1:]\n",
    "    \n",
    "    selected_GMs = [old_global_models[ii] for ii in GM_intv]\n",
    "    selected_CMs = [old_client_models[jj] for jj in CM_intv]\n",
    "    \n",
    "    \n",
    "    \"\"\"1. First, complete the model overlay from the initial model to the first round of global train\"\"\"\n",
    "    \"\"\"\n",
    "    Since the inIT_model does not contain any information about the forgotten user at the start of the FL training, you just need to overlay the local Model of the other retained users, You can get the Global Model after the first round of global training.\n",
    "    \"\"\"\n",
    "    epoch = 0\n",
    "    unlearn_global_models = list()\n",
    "    unlearn_global_models.append(copy.deepcopy(selected_GMs[0]))\n",
    "    \n",
    "    new_global_model = fedavg(selected_CMs[epoch])\n",
    "    unlearn_global_models.append(copy.deepcopy(new_global_model))\n",
    "    print(\"Federated Unlearning Global Epoch  = {}\".format(epoch))\n",
    "    \n",
    "    \"\"\"2. Then, the first round of global model as a starting point, the model is gradually corrected\"\"\"\n",
    "    \"\"\"\n",
    "    In this step, the global Model obtained from the first round of global training was used as the new starting point for training, and a small amount of training was carried out with the data of the reserved user (a small amount means reducing the local epoch, i.e. Reduce the number of local training rounds for each user. The parameter forget_local_epoch_ratio is to control and reduce the number of local training rounds.) Gets the direction of iteration of the local Model parameter for each reserved user, starting with new_global_model.Note that this part of the user model is ref_client_models.\n",
    "    \n",
    "    Then we use the old_client_models and old_global_models saved from the unforgotten FL training, and the ref_client_models and new_global_Model that we get when we forget a user,To build the global model for the next round\n",
    "    \n",
    "    \n",
    "    (ref_client_models - new_global_model) / ||ref_client_models - new_global_model||，Indicates the direction of model parameter iteration starting with a new global model that removes a user.Mark the direction as step_direction\n",
    "\n",
    "    ||old_client_models - old_global_model||，Indicates the step size of the model parameter iteration starting with the old global model with a user removed.Step step_length\n",
    "    \n",
    "    So, the final direction of the new reference model is step_direction*step_length + new_global_model。\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Intuitive explanation of this part: Usually in IID data, after the data is sharded, the direction of model parameter iteration is roughly the same.The basic idea is to take full advantage of the client-model parameter data saved in standard FL training, and then, by correcting this part of the parameter, apply it to the iteration of the new global model that forgets a user.\n",
    "    \n",
    "    For unforgotten FL:oldGM_t--> oldCM0, oldCM1, oldCM2, oldCM3--> oldGM_t+1\n",
    "    for unblearning FL：newGM_t-->newCM0, newCM1, newCM2, newCM3--> newGM_t+1\n",
    "    oldGM_t and newGM_t essentially represents a different starting point for training. However, under the IID data, oldCM and newCM should converge in roughly the same direction.\n",
    "    Therefore, we get newCM by using newcm-newgm_t as the starting point and training fewer rounds on user data, and then using (newcm-newgm_t)/|| newcm-newgm_t || as the current forgetting setting,\n",
    "    Direction of model parameter iteration.Take || oldcm-oldgm_t || as the iteration step, and finally use || oldcm-oldgm_t ||*(newcm-newgm_t)/|| newcm-newgm_t |0 |1 for the iteration of the new model.\n",
    "    FedEraser iterative formula: newGM_t+1 = newGM_t + ||oldCM - oldGM_t||*(newCM - newGM_t)/||newCM - newGM_t||\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    CONST_local_epoch = copy.deepcopy(FL_params.local_epoch)\n",
    "    FL_params.local_epoch = np.ceil(FL_params.local_epoch*FL_params.forget_local_epoch_ratio)\n",
    "    FL_params.local_epoch = np.int16(FL_params.local_epoch)\n",
    "\n",
    "    CONST_global_epoch = copy.deepcopy(FL_params.global_epoch)\n",
    "    FL_params.global_epoch = CM_intv.shape[0]\n",
    "    \n",
    "    \n",
    "    print('Local Calibration Training epoch = {}'.format(FL_params.local_epoch))\n",
    "    for epoch in range(FL_params.global_epoch):\n",
    "        if(epoch == 0):\n",
    "            continue\n",
    "        print(\"Federated Unlearning Global Epoch  = {}\".format(epoch))\n",
    "        global_model = unlearn_global_models[epoch]\n",
    "\n",
    "        new_client_models  = global_train_once(global_model, client_data_loaders, test_loader, FL_params)\n",
    "\n",
    "        new_GM = unlearning_step_once(selected_CMs[epoch], new_client_models, selected_GMs[epoch+1], global_model)\n",
    "        \n",
    "        unlearn_global_models.append(new_GM)\n",
    "    FL_params.local_epoch = CONST_local_epoch\n",
    "    FL_params.global_epoch = CONST_global_epoch\n",
    "    return unlearn_global_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unlearning_without_cali(old_global_models, old_client_models, FL_params):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    old_client_models : list of DNN models\n",
    "        All user local update models are saved during the federated learning and training process that is not forgotten.\n",
    "    FL_params : parameters\n",
    "        All parameters in federated learning and federated forgetting learning\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    global_models : List of DNN models\n",
    "        In each update round, the client model of the user who needs to be forgotten is removed, and the parameters of other users' client models are directly superimposing to form the new Global Model of each round\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    The basic process is as follows：For unforgotten FL:oldGM_t--> oldCM0, oldCM1, oldCM2, oldCM3--> oldGM_t+1\n",
    "                 For unlearning FL：newGM_t-->The parameters of oldCM and oldGM were directly leveraged to update global model--> newGM_t+1\n",
    "    The update process is as follows：newGM_t+1 = (oldCM - oldGM_t) + newGM_t\n",
    "    \"\"\"\n",
    "    if(FL_params.if_unlearning == False):\n",
    "        raise ValueError('FL_params.if_unlearning should be set to True, if you want to unlearning with a certain user')\n",
    "        \n",
    "    if(not(FL_params.forget_client_idx in range(FL_params.N_client))):\n",
    "        raise ValueError('FL_params.forget_client_idx is note assined correctly, forget_client_idx should in {}'.format(range(FL_params.N_client)))\n",
    "    forget_client = FL_params.forget_client_idx\n",
    "    \n",
    "    \n",
    "    for ii in range(FL_params.global_epoch):\n",
    "        temp = old_client_models[ii*FL_params.N_client : ii*FL_params.N_client+FL_params.N_client]\n",
    "        temp.pop(forget_client)\n",
    "        old_client_models.append(temp)\n",
    "    old_client_models = old_client_models[-FL_params.global_epoch:]\n",
    "    \n",
    "    uncali_global_models = list()\n",
    "    uncali_global_models.append(copy.deepcopy(old_global_models[0]))\n",
    "    epoch = 0\n",
    "    uncali_global_model = fedavg(old_client_models[epoch])\n",
    "    uncali_global_models.append(copy.deepcopy(uncali_global_model))\n",
    "    print(\"Federated Unlearning without Clibration Global Epoch  = {}\".format(epoch))\n",
    "    \n",
    "    \"\"\"\n",
    "    new_GM_t+1 = newGM_t + (oldCM_t - oldGM_t)\n",
    "    \n",
    "    For standard federated learning:oldGM_t --> oldCM_t --> oldGM_t+1\n",
    "    For accumulatring:    newGM_t --> (oldCM_t - oldGM_t) --> oldGM_t+1\n",
    "    For uncalibrated federated forgotten learning, the parameter update of the unforgotten user in standard federated learning is used to directly overlay the new global model to obtain the next round of new global model.\n",
    "    \"\"\"\n",
    "    old_param_update = dict()#(oldCM_t - oldGM_t)\n",
    "    return_model_state = dict()#newGM_t+1\n",
    "    \n",
    "    for epoch in range(FL_params.global_epoch):\n",
    "        if(epoch == 0):\n",
    "            continue\n",
    "        print(\"Federated Unlearning Global Epoch  = {}\".format(epoch))\n",
    "        \n",
    "        current_global_model = uncali_global_models[epoch]#newGM_t\n",
    "        current_client_models = old_client_models[epoch]#oldCM_t\n",
    "        old_global_model = old_global_models[epoch]#oldGM_t\n",
    "        # global_model_before_forget = old_global_models[epoch]#old_GM_t\n",
    "        \n",
    "        \n",
    "        for layer in current_global_model.state_dict().keys():\n",
    "            #State variable initialization\n",
    "            old_param_update[layer] = 0*current_global_model.state_dict()[layer]\n",
    "            return_model_state[layer] = 0*current_global_model.state_dict()[layer]\n",
    "            \n",
    "            for ii in range(len(current_client_models)):\n",
    "                old_param_update[layer] += current_client_models[ii].state_dict()[layer]\n",
    "            old_param_update[layer] /= (ii+1)# oldCM_t\n",
    "            \n",
    "            old_param_update[layer] = old_param_update[layer] - old_global_model.state_dict()[layer]#参数： oldCM_t - oldGM_t\n",
    "\n",
    "            return_model_state[layer] = current_global_model.state_dict()[layer] + old_param_update[layer]#newGM_t + (oldCM_t - oldGM_t)\n",
    "            \n",
    "        return_global_model = copy.deepcopy(old_global_models[0])\n",
    "        return_global_model.load_state_dict(return_model_state)\n",
    "            \n",
    "        uncali_global_models.append(return_global_model)\n",
    "\n",
    "    return uncali_global_models\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def federated_learning_unlearning(init_global_model, client_loaders, test_loader, FL_params):\n",
    "    \n",
    "    \n",
    "    # all_global_models, all_client_models 为保存起来所有的old FL models\n",
    "    print(5*\"#\"+\"  Federated Learning Start\"+5*\"#\")\n",
    "    std_time = time.time()\n",
    "    old_GMs, old_CMs = FL_Train(init_global_model, client_loaders, test_loader, FL_params)\n",
    "    end_time = time.time()\n",
    "    time_learn = (std_time - end_time)\n",
    "    print(5*\"#\"+\"  Federated Learning End\"+5*\"#\")\n",
    "    \n",
    "    \n",
    "    print('\\n')\n",
    "    \"\"\"4.2 unlearning  a client，Federated Unlearning\"\"\"\n",
    "    print(5*\"#\"+\"  Federated Unlearning Start  \"+5*\"#\")\n",
    "    std_time = time.time()\n",
    "    #Set the parameter IF_unlearning =True so that global_train_once skips forgotten users and saves computing time\n",
    "    FL_params.if_unlearning = True\n",
    "    #Set the parameter forget_client_IDx to mark the user's IDX that needs to be forgotten\n",
    "    FL_params.forget_client_idx = 2\n",
    "    unlearn_GMs = unlearning(old_GMs, old_CMs, client_loaders, test_loader, FL_params)\n",
    "    end_time = time.time()\n",
    "    time_unlearn = (std_time - end_time)\n",
    "    print(5*\"#\"+\"  Federated Unlearning End  \"+5*\"#\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('\\n')\n",
    "    \"\"\"4.3 unlearning a client，Federated Unlearning without calibration\"\"\"\n",
    "    print(5*\"#\"+\"  Federated Unlearning without Calibration Start  \"+5*\"#\")\n",
    "    std_time = time.time()\n",
    "    uncali_unlearn_GMs = unlearning_without_cali(old_GMs, old_CMs, FL_params)\n",
    "    end_time = time.time()\n",
    "    time_unlearn_no_cali = (std_time - end_time)\n",
    "    print(5*\"#\"+\"  Federated Unlearning without Calibration End  \"+5*\"#\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if(FL_params.if_retrain):\n",
    "    #     print('\\n')\n",
    "    #     print(5*\"#\"+\"  Federated Retraining Start  \"+5*\"#\")\n",
    "    #     std_time = time.time()\n",
    "    #     # FL_params.N_client = FL_params.N_client - 1\n",
    "    #     # client_loaders.pop(FL_params.forget_client_idx)\n",
    "    #     # retrain_GMs, _ = FL_Train(init_global_model, client_loaders, test_loader, FL_params)\n",
    "    #     retrain_GMs = FL_Retrain(init_global_model, client_loaders, test_loader, FL_params)\n",
    "    #     end_time = time.time()\n",
    "    #     time_retrain = (std_time - end_time)\n",
    "    #     print(5*\"#\"+\"  Federated Retraining End  \"+5*\"#\")\n",
    "    # else:\n",
    "    #     print('\\n')\n",
    "    #     print(5*\"#\"+\"  No Retraining \"+5*\"#\")\n",
    "    #     retrain_GMs = list()\n",
    "    \n",
    "    print(\" Learning time consuming = {} secods\".format(-time_learn))\n",
    "    print(\" Unlearning time consuming = {} secods\".format(-time_unlearn)) \n",
    "    print(\" Unlearning no Cali time consuming = {} secods\".format(-time_unlearn_no_cali)) \n",
    "    # print(\" Retraining time consuming = {} secods\".format(-time_retrain)) \n",
    "    \n",
    "    \n",
    "    return old_GMs, unlearn_GMs, uncali_unlearn_GMs, old_CMs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_attack_model(shadow_old_GM, shadow_client_loaders, shadow_test_loader, FL_params):\n",
    "    shadow_model = shadow_old_GM\n",
    "    n_class_dict = dict()\n",
    "    n_class_dict['adult'] = 2\n",
    "    n_class_dict['purchase'] = 2\n",
    "    n_class_dict['mnist'] = 10\n",
    "    n_class_dict['cifar10'] = 10\n",
    "    \n",
    "    N_class = n_class_dict[FL_params.data_name]\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    shadow_model.to(device)\n",
    "        \n",
    "    shadow_model.eval()\n",
    "    ####\n",
    "    pred_4_mem = torch.zeros([1,N_class])\n",
    "    pred_4_mem = pred_4_mem.to(device)\n",
    "    with torch.no_grad():\n",
    "        for ii in range(len(shadow_client_loaders)):\n",
    "            # if(ii != FL_params.forget_client_idx):\n",
    "            #     continue\n",
    "            data_loader = shadow_client_loaders[ii]\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(data_loader):\n",
    "                    data = data.to(device)\n",
    "                    out = shadow_model(data)\n",
    "                    pred_4_mem = torch.cat([pred_4_mem, out])\n",
    "    pred_4_mem = pred_4_mem[1:,:]\n",
    "    pred_4_mem = softmax(pred_4_mem,dim = 1)\n",
    "    pred_4_mem = pred_4_mem.cpu()\n",
    "    pred_4_mem = pred_4_mem.detach().numpy()\n",
    "    \n",
    "    ####\n",
    "    pred_4_nonmem = torch.zeros([1,N_class])\n",
    "    pred_4_nonmem = pred_4_nonmem.to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch, (data, target) in enumerate(shadow_test_loader):\n",
    "            data = data.to(device)\n",
    "            out = shadow_model(data)\n",
    "            pred_4_nonmem = torch.cat([pred_4_nonmem, out])\n",
    "    pred_4_nonmem = pred_4_nonmem[1:,:]\n",
    "    pred_4_nonmem = softmax(pred_4_nonmem,dim = 1)\n",
    "    pred_4_nonmem = pred_4_nonmem.cpu()\n",
    "    pred_4_nonmem = pred_4_nonmem.detach().numpy()\n",
    "    \n",
    "    \n",
    "    #构建MIA 攻击模型 \n",
    "    att_y = np.hstack((np.ones(pred_4_mem.shape[0]), np.zeros(pred_4_nonmem.shape[0])))\n",
    "    att_y = att_y.astype(np.int16)\n",
    "    \n",
    "    att_X = np.vstack((pred_4_mem, pred_4_nonmem))\n",
    "    att_X.sort(axis=1)\n",
    "    \n",
    "    X_train,X_test, y_train, y_test = train_test_split(att_X, att_y, test_size = 0.1)\n",
    "    \n",
    "    attacker = XGBClassifier(n_estimators = 300,\n",
    "                              n_jobs = -1,\n",
    "                                max_depth = 30,\n",
    "                              objective = 'binary:logistic',\n",
    "                              booster=\"gbtree\",\n",
    "                              # learning_rate=None,\n",
    "                               # tree_method = 'gpu_hist',\n",
    "                               scale_pos_weight = pred_4_nonmem.shape[0]/pred_4_mem.shape[0]\n",
    "                              )\n",
    "    \n",
    "\n",
    "    \n",
    "    attacker.fit(X_train, y_train)\n",
    "    # print('\\n')\n",
    "    # print(\"MIA Attacker training accuracy\")\n",
    "    # print(accuracy_score(y_train, attacker.predict(X_train)))\n",
    "    # print(\"MIA Attacker testing accuracy\")\n",
    "    # print(accuracy_score(y_test, attacker.predict(X_test)))\n",
    "    \n",
    "    return attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def attack(target_model, attack_model, client_loaders, test_loader, FL_params):\n",
    "    n_class_dict = dict()\n",
    "    n_class_dict['adult'] = 2\n",
    "    n_class_dict['purchase'] = 2\n",
    "    n_class_dict['mnist'] = 10\n",
    "    n_class_dict['cifar10'] = 10\n",
    "    \n",
    "    N_class = n_class_dict[FL_params.data_name]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    \n",
    "    target_model.to(device)\n",
    "        \n",
    "    target_model.eval()\n",
    "    \n",
    "    #The predictive output of forgotten user data after passing through the target model.\n",
    "    unlearn_X = torch.zeros([1,N_class])\n",
    "    unlearn_X = unlearn_X.to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(client_loaders[FL_params.forget_client_idx]):\n",
    "                    data = data.to(device)\n",
    "                    out = target_model(data)\n",
    "                    unlearn_X = torch.cat([unlearn_X, out])\n",
    "                    \n",
    "    unlearn_X = unlearn_X[1:,:]\n",
    "    unlearn_X = softmax(unlearn_X,dim = 1)\n",
    "    unlearn_X = unlearn_X.cpu().detach().numpy()\n",
    "    \n",
    "    unlearn_X.sort(axis=1)\n",
    "    unlearn_y = np.ones(unlearn_X.shape[0])\n",
    "    unlearn_y = unlearn_y.astype(np.int16)\n",
    "    \n",
    "    N_unlearn_sample = len(unlearn_y)\n",
    "    \n",
    "    #Test data, predictive output obtained after passing the target model\n",
    "    test_X = torch.zeros([1, N_class])\n",
    "    test_X = test_X.to(device)\n",
    "    with torch.no_grad():\n",
    "        for _, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            out = target_model(data)\n",
    "            test_X = torch.cat([test_X, out])\n",
    "            \n",
    "            if(test_X.shape[0] > N_unlearn_sample):\n",
    "                break\n",
    "    test_X = test_X[1:N_unlearn_sample+1,:]\n",
    "    test_X = softmax(test_X,dim = 1)\n",
    "    test_X = test_X.cpu().detach().numpy()\n",
    "    \n",
    "    test_X.sort(axis=1)\n",
    "    test_y = np.zeros(test_X.shape[0])\n",
    "    test_y = test_y.astype(np.int16)\n",
    "    \n",
    "    #The data of the forgotten user passed through the output of the target model, and the data of the test set passed through the output of the target model were spliced together\n",
    "    #The balanced data set that forms the 50% train 50% test.\n",
    "    XX = np.vstack((unlearn_X, test_X))\n",
    "    YY = np.hstack((unlearn_y, test_y))\n",
    "    \n",
    "    pred_YY = attack_model.predict(XX)\n",
    "    # acc = accuracy_score( YY, pred_YY)\n",
    "    pre = precision_score(YY, pred_YY, pos_label=1)\n",
    "    rec = recall_score(YY, pred_YY, pos_label=1)\n",
    "    # print(\"MIA Attacker accuracy = {:.4f}\".format(acc))\n",
    "    print(\"MIA Attacker precision = {:.4f}\".format(pre))\n",
    "    print(\"MIA Attacker recall = {:.4f}\".format(rec))\n",
    "    \n",
    "    return (pre, rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super-parameters setting\n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        #Federated Learning Settings\n",
    "        self.N_total_client = 100\n",
    "        self.N_client = 10\n",
    "        \n",
    "        self.data_name = 'mnist' # optional datasets such as (purchase, cifar10, mnist, adult)\n",
    "        \n",
    "        self.global_epoch = 20\n",
    "        self.local_epoch = 10\n",
    "        \n",
    "        #Model Training Settings\n",
    "        self.local_batch_size = 64\n",
    "        self.local_lr = 0.005\n",
    "        \n",
    "        self.test_batch_size = 64\n",
    "        self.seed = 1\n",
    "        self.save_all_model = True\n",
    "        self.cuda_state = torch.cuda.is_available()\n",
    "        self.use_gpu = True\n",
    "        self.train_with_test = False\n",
    "        \n",
    "        #Federated Unlearning Settings\n",
    "        self.unlearn_interval= 1\n",
    "        #Used to control how many rounds the model parameters are saved.1 represents the parameter saved once per round  N_itv in our paper.\n",
    "        self.forget_client_idx = 2 \n",
    "        #If want to forget, change None to the client index\n",
    "        \n",
    "        #If this parameter is set to False, only the global model after the final training is completed is output\n",
    "        #If set to True, the global model is retrained using the FL-Retrain function, and data corresponding to the user for the forget_client_IDx number is discarded.\n",
    "        self.if_retrain = False\n",
    "        \n",
    "        #If set to False, the global_train_once function will not skip users that need to be forgotten;If set to True, global_train_once skips the forgotten user during training\n",
    "        self.if_unlearning = False\n",
    "        \n",
    "        self.forget_local_epoch_ratio = 0.5 \n",
    "        #When a user is selected to be forgotten, other users need to train several rounds of on-line training in their respective data sets to obtain the general direction of model convergence in order to provide the general direction of model convergence.\n",
    "        #forget_local_epoch_ratio*local_epoch Is the number of rounds of local training when we need to get the convergence direction of each local model\n",
    "        # self.mia_oldGM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "FL_params = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Step1. Federated Learning Settings \n",
      " We use dataset: mnist for our Federated Unlearning experiment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(FL_params.seed)\n",
    "print(60*'=')\n",
    "print(\"Step1. Federated Learning Settings \\n We use dataset: \"+FL_params.data_name+(\" for our Federated Unlearning experiment.\\n\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Step2. Client data loaded, testing data loaded!!!\n",
      "       Initial Model loaded!!!\n"
     ]
    }
   ],
   "source": [
    "print(60*'=')\n",
    "print(\"Step2. Client data loaded, testing data loaded!!!\\n       Initial Model loaded!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_global_model = model_init(FL_params.data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_mnist(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mnist model\n",
    "init_global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_all_loaders, test_loader = data_init(FL_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_clients = np.random.choice(range(FL_params.N_total_client),size=FL_params.N_client, replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 41, 97, 44, 89, 87, 53, 50, 23, 25])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_loaders = list()\n",
    "for idx in selected_clients:\n",
    "    client_loaders.append(client_all_loaders[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Step3. Fedearated Learning and Unlearning Training...\n"
     ]
    }
   ],
   "source": [
    "print(60*'=')\n",
    "print(\"Step3. Fedearated Learning and Unlearning Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####  Federated Learning Start#####\n",
      "Global Federated Learning epoch = 0\n",
      "Global Federated Learning epoch = 1\n",
      "Global Federated Learning epoch = 2\n",
      "Global Federated Learning epoch = 3\n",
      "Global Federated Learning epoch = 4\n",
      "Global Federated Learning epoch = 5\n",
      "Global Federated Learning epoch = 6\n",
      "Global Federated Learning epoch = 7\n",
      "Global Federated Learning epoch = 8\n",
      "Global Federated Learning epoch = 9\n",
      "Global Federated Learning epoch = 10\n",
      "Global Federated Learning epoch = 11\n",
      "Global Federated Learning epoch = 12\n",
      "Global Federated Learning epoch = 13\n",
      "Global Federated Learning epoch = 14\n",
      "Global Federated Learning epoch = 15\n",
      "Global Federated Learning epoch = 16\n",
      "Global Federated Learning epoch = 17\n",
      "Global Federated Learning epoch = 18\n",
      "Global Federated Learning epoch = 19\n",
      "#####  Federated Learning End#####\n",
      "\n",
      "\n",
      "#####  Federated Unlearning Start  #####\n",
      "Federated Unlearning Global Epoch  = 0\n",
      "Local Calibration Training epoch = 5\n",
      "Federated Unlearning Global Epoch  = 1\n",
      "Federated Unlearning Global Epoch  = 2\n",
      "Federated Unlearning Global Epoch  = 3\n",
      "Federated Unlearning Global Epoch  = 4\n",
      "Federated Unlearning Global Epoch  = 5\n",
      "Federated Unlearning Global Epoch  = 6\n",
      "Federated Unlearning Global Epoch  = 7\n",
      "Federated Unlearning Global Epoch  = 8\n",
      "Federated Unlearning Global Epoch  = 9\n",
      "Federated Unlearning Global Epoch  = 10\n",
      "Federated Unlearning Global Epoch  = 11\n",
      "Federated Unlearning Global Epoch  = 12\n",
      "Federated Unlearning Global Epoch  = 13\n",
      "Federated Unlearning Global Epoch  = 14\n",
      "Federated Unlearning Global Epoch  = 15\n",
      "Federated Unlearning Global Epoch  = 16\n",
      "Federated Unlearning Global Epoch  = 17\n",
      "Federated Unlearning Global Epoch  = 18\n",
      "Federated Unlearning Global Epoch  = 19\n",
      "#####  Federated Unlearning End  #####\n",
      "\n",
      "\n",
      "#####  Federated Unlearning without Calibration Start  #####\n",
      "Federated Unlearning without Clibration Global Epoch  = 0\n",
      "Federated Unlearning Global Epoch  = 1\n",
      "Federated Unlearning Global Epoch  = 2\n",
      "Federated Unlearning Global Epoch  = 3\n",
      "Federated Unlearning Global Epoch  = 4\n",
      "Federated Unlearning Global Epoch  = 5\n",
      "Federated Unlearning Global Epoch  = 6\n",
      "Federated Unlearning Global Epoch  = 7\n",
      "Federated Unlearning Global Epoch  = 8\n",
      "Federated Unlearning Global Epoch  = 9\n",
      "Federated Unlearning Global Epoch  = 10\n",
      "Federated Unlearning Global Epoch  = 11\n",
      "Federated Unlearning Global Epoch  = 12\n",
      "Federated Unlearning Global Epoch  = 13\n",
      "Federated Unlearning Global Epoch  = 14\n",
      "Federated Unlearning Global Epoch  = 15\n",
      "Federated Unlearning Global Epoch  = 16\n",
      "Federated Unlearning Global Epoch  = 17\n",
      "Federated Unlearning Global Epoch  = 18\n",
      "Federated Unlearning Global Epoch  = 19\n",
      "#####  Federated Unlearning without Calibration End  #####\n",
      " Learning time consuming = 85.1621298789978 secods\n",
      " Unlearning time consuming = 33.19119882583618 secods\n",
      " Unlearning no Cali time consuming = 0.09371662139892578 secods\n"
     ]
    }
   ],
   "source": [
    "old_GMs, unlearn_GMs, uncali_unlearn_GMs, _ = \\\n",
    "    federated_learning_unlearning(\n",
    "                                    init_global_model, \n",
    "                                    client_loaders, \n",
    "                                    test_loader, \n",
    "                                    FL_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(FL_params.if_retrain == True):\n",
    "    t1 = time.time()\n",
    "    retrain_GMs = FL_Retrain(init_global_model, client_loaders, test_loader, FL_params)\n",
    "    t2 = time.time()\n",
    "    print(\"Time using = {} seconds\".format(t2-t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Step4. Membership Inference Attack aganist GM...\n"
     ]
    }
   ],
   "source": [
    "print(60*'=')\n",
    "print(\"Step4. Membership Inference Attack aganist GM...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  = -1\n",
      "Attacking against FL Standard  \n"
     ]
    }
   ],
   "source": [
    "T_epoch = -1\n",
    "# MIA setting:Target model == Shadow Model\n",
    "old_GM = old_GMs[T_epoch]\n",
    "attack_model = train_attack_model(old_GM, client_loaders, test_loader, FL_params)\n",
    "\n",
    "\n",
    "print(\"\\nEpoch  = {}\".format(T_epoch))\n",
    "print(\"Attacking against FL Standard  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIA Attacker precision = 0.9640\n",
      "MIA Attacker recall = 0.9383\n"
     ]
    }
   ],
   "source": [
    "target_model = old_GMs[T_epoch]\n",
    "(ACC_old, PRE_old) = attack(target_model, attack_model, client_loaders, test_loader, FL_params)\n",
    "\n",
    "if(FL_params.if_retrain == True):\n",
    "    print(\"Attacking against FL Retrain  \")\n",
    "    target_model = retrain_GMs[T_epoch]\n",
    "    (ACC_retrain, PRE_retrain) = attack(target_model, attack_model, client_loaders, test_loader, FL_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attacking against FL Unlearn  \n",
      "MIA Attacker precision = 0.5126\n",
      "MIA Attacker recall = 0.4067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Attacking against FL Unlearn  \")\n",
    "target_model = unlearn_GMs[T_epoch]\n",
    "(ACC_unlearn, PRE_unlearn) = attack(target_model, attack_model, client_loaders, test_loader, FL_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adver_attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
