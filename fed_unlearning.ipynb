{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定超参数\n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.N_total_client = 100  # 总客户端数量\n",
    "        self.N_client = 10  # 参与训练的客户端数量\n",
    "        # 可选数据集，例如 (purchase, cifar10, mnist, adult)\n",
    "        self.data_name = 'mnist'  \n",
    "        # 全局训练周期数\n",
    "        self.global_epoch = 20  \n",
    "        # 局部训练周期数\n",
    "        self.local_epoch = 10  \n",
    "        # 批处理大小\n",
    "        self.local_batch_size = 64  # 局部批处理大小\n",
    "        self.test_batch_size = 64  # 测试批处理大小\n",
    "        # 学习率\n",
    "        self.local_lr = 0.005  # 局部学习率\n",
    "        \n",
    "        self.seed = 1  # 随机种子\n",
    "        self.save_all_model = True  # 保存所有模型\n",
    "        self.cuda_state = torch.cuda.is_available()  # 检查CUDA是否可用\n",
    "        self.use_gpu = True  # 使用GPU\n",
    "        self.train_with_test = False  # 用测试集训练\n",
    "        \n",
    "        # 联邦遗忘设置\n",
    "        self.unlearn_interval= 1  # 遗忘间隔\n",
    "        # 用于控制模型参数保存的轮数。1表示每轮保存一次参数，N_itv是我们论文中的表示。\n",
    "        self.forget_client_idx = 2  \n",
    "        # 如果想要遗忘，将None更改为客户端索引\n",
    "        # 如果此参数设置为False，仅在最终训练完成后输出全局模型\n",
    "        # 如果设置为True，则使用FL-Retrain函数重新训练全局模型，并丢弃与forget_client_IDx号用户对应的数据。\n",
    "        self.if_retrain = False  \n",
    "        # 如果设置为False，global_train_once函数在训练过程中不会跳过需要遗忘的用户；如果设置为True，global_train_once在训练过程中跳过遗忘的用户\n",
    "        self.if_unlearning = False  \n",
    "        self.forget_local_epoch_ratio = 0.5  \n",
    "        # 当选择遗忘用户时，\n",
    "        # 其他用户需要在各自的数据集中进行几轮在线训练，以获得模型收敛的一般方向，\n",
    "        # 以便为模型收敛提供一般方向。\n",
    "        # forget_local_epoch_ratio*local_epoch是我们需要获得每个局部模型收敛方向时的局部训练轮数\n",
    "        # self.mia_oldGM = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Step1. Federated Learning Settings \n",
      " We use dataset: mnist for our Federated Unlearning experiment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step1. super-parameter setting\n",
    "FL_params = Arguments()\n",
    "torch.manual_seed(FL_params.seed)\n",
    "print(60*'=')\n",
    "print(\"Step1. Federated Learning Settings \\n We use dataset: \"+FL_params.data_name+(\" for our Federated Unlearning experiment.\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Step2. Client data loaded, testing data loaded!!!\n",
      "       Initial Model loaded!!!\n"
     ]
    }
   ],
   "source": [
    "print(60*'=')\n",
    "print(\"Step2. Client data loaded, testing data loaded!!!\\n       Initial Model loaded!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型初始化\n",
    "def model_init(data_name):\n",
    "    if(data_name == 'mnist'):\n",
    "        model = Net_mnist()\n",
    "    elif(data_name == 'cifar10'):\n",
    "        model = Net_cifar10()\n",
    "    elif(data_name == 'purchase'):\n",
    "        model = Net_purchase()\n",
    "    elif(data_name == 'adult'):\n",
    "        model = Net_adult()\n",
    "    return model\n",
    "\n",
    "# 以下是不同数据集的模型框架\n",
    "class Net_mnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_mnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Net_purchase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_purchase, self).__init__()\n",
    "        self.fc1 = nn.Linear(600, 300)\n",
    "        self.fc2 = nn.Linear(300, 50)\n",
    "        self.fc3 = nn.Linear(50, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x   \n",
    "    \n",
    "\n",
    "class Net_adult(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_adult, self).__init__()\n",
    "        self.fc1 = nn.Linear(108, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x     \n",
    "\n",
    "\n",
    "\n",
    "class Net_cifar10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_cifar10, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_mnist(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化模型\n",
    "init_global_model = model_init(FL_params.data_name)\n",
    "init_global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_init(FL_params):\n",
    "    \n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True} if FL_params.cuda_state else {}\n",
    "    trainset, testset = data_set(FL_params.data_name) # 获得训练集、测试集\n",
    "    \n",
    "    # 构建测试数据加载器\n",
    "    test_loader = DataLoader(testset, batch_size=FL_params.test_batch_size, shuffle=True, **kwargs)                \n",
    "    \n",
    "    # 将数据按照训练的trainset，均匀的分配成N-client份，所有分割得到dataset都保存在一个list中\n",
    "    split_index = [int(trainset.__len__()/FL_params.N_total_client)]*(FL_params.N_total_client-1) \n",
    "    split_index.append(int(trainset.__len__() - int(trainset.__len__()/FL_params.N_total_client)*(FL_params.N_total_client-1)))\n",
    "    client_dataset = torch.utils.data.random_split(trainset, split_index)\n",
    "    \n",
    "    # 将全局模型复制N-client次，然后构建每一个client模型的优化器，参数记录   \n",
    "    client_loaders = []\n",
    "    for tt in range(FL_params.N_total_client):\n",
    "        client_loaders.append(DataLoader(client_dataset[tt], FL_params.local_batch_size, shuffle=True, **kwargs))\n",
    "    \n",
    "    return client_loaders, test_loader\n",
    "\n",
    "\n",
    "def data_set(data_name):\n",
    "    if not data_name in ['mnist','purchase','adult','cifar10']:\n",
    "        raise TypeError('data_name should be a string, including mnist,purchase,adult,cifar10. ')\n",
    "    \n",
    "    # model: 2 conv. layers followed by 2 FC layers\n",
    "    if data_name == 'mnist':\n",
    "        trainset = datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "        testset = datasets.MNIST('./data', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "        \n",
    "    # model: ResNet-50\n",
    "    elif data_name == 'cifar10':\n",
    "        transform = transforms.Compose(\n",
    "                                        [transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                        ])\n",
    "        \n",
    "        trainset = datasets.CIFAR10(root='./data', \n",
    "                                    train=True,\n",
    "                                    download=True, \n",
    "                                    transform=transform)\n",
    "        \n",
    "        testset = datasets.CIFAR10(root='./data', \n",
    "                                   train=False,\n",
    "                                   download=True, \n",
    "                                   transform=transform)\n",
    "        \n",
    "        \n",
    "    # model: 2 FC layers\n",
    "    elif data_name == 'purchase':\n",
    "        xx = np.load(\"./data/purchase/purchase_xx.npy\")\n",
    "        yy = np.load(\"./data/purchase/purchase_y2.npy\")\n",
    "     \n",
    "        X_train, X_test, y_train, y_test = train_test_split(xx, yy, test_size=0.2, random_state=42)\n",
    "        \n",
    "        X_train_tensor = torch.Tensor(X_train).type(torch.FloatTensor)\n",
    "        X_test_tensor = torch.Tensor(X_test).type(torch.FloatTensor)\n",
    "        y_train_tensor = torch.Tensor(y_train).type(torch.LongTensor)\n",
    "        y_test_tensor = torch.Tensor(y_test).type(torch.LongTensor)\n",
    "        \n",
    "        trainset = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "        testset = TensorDataset(X_test_tensor,y_test_tensor)\n",
    "    \n",
    "    # model: 2 FC layers\n",
    "    elif data_name == 'adult':\n",
    "        #load data\n",
    "        file_path = \"./data/adult/\"\n",
    "        data1 = pd.read_csv(file_path + 'adult.data', header=None)\n",
    "        data2 = pd.read_csv(file_path + 'adult.test', header=None)\n",
    "        data2 = data2.replace(' <=50K.', ' <=50K')    \n",
    "        data2 = data2.replace(' >50K.', ' >50K')\n",
    "        train_num = data1.shape[0]\n",
    "        data = pd.concat([data1,data2])\n",
    "        #data transform: str->int\n",
    "        data = np.array(data, dtype=str)\n",
    "        labels = data[:,14]\n",
    "        le= LabelEncoder()\n",
    "        le.fit(labels)\n",
    "        labels = le.transform(labels)\n",
    "        data = data[:,:-1]\n",
    "        \n",
    "        categorical_features = [1,3,5,6,7,8,9,13]\n",
    "        # categorical_names = {}\n",
    "        for feature in categorical_features:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(data[:, feature])\n",
    "            data[:, feature] = le.transform(data[:, feature])\n",
    "            # categorical_names[feature] = le.classes_\n",
    "        data = data.astype(float)\n",
    "        \n",
    "        n_features = data.shape[1]\n",
    "        numerical_features = list(set(range(n_features)).difference(set(categorical_features)))\n",
    "        for feature in numerical_features:\n",
    "            scaler = MinMaxScaler()\n",
    "            sacled_data = scaler.fit_transform(data[:,feature].reshape(-1,1))\n",
    "            data[:,feature] = sacled_data.reshape(-1)\n",
    "        \n",
    "        #OneHotLabel\n",
    "        oh_encoder = ColumnTransformer(\n",
    "            [('oh_enc', OneHotEncoder(sparse=False), categorical_features),], \n",
    "            remainder='passthrough' )\n",
    "        oh_data = oh_encoder.fit_transform(data)\n",
    "        \n",
    "        xx = oh_data\n",
    "        yy = labels\n",
    "        #最终处理，xx进行规范化\n",
    "        xx = preprocessing.scale(xx)\n",
    "        yy = np.array(yy)\n",
    "        xx = torch.Tensor(xx).type(torch.FloatTensor)\n",
    "        yy = torch.Tensor(yy).type(torch.LongTensor)\n",
    "        xx_train = xx[0:data1.shape[0],:]\n",
    "        xx_test = xx[data1.shape[0]:,:]\n",
    "        yy_train = yy[0:data1.shape[0]]\n",
    "        yy_test = yy[data1.shape[0]:]\n",
    "\n",
    "        trainset = TensorDataset(xx_train,yy_train)\n",
    "        testset = TensorDataset(xx_test,yy_test)\n",
    "        \n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分每一个client的数据集\n",
    "client_all_loaders, test_loader = data_init(FL_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 58, 66, 72, 11, 14, 73, 20,  0, 99])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随机抽取client\n",
    "selected_clients = np.random.choice(range(FL_params.N_total_client),size=FL_params.N_client, replace=False)\n",
    "selected_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_loaders = list()\n",
    "for idx in selected_clients:\n",
    "    client_loaders.append(client_all_loaders[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Step3. Fedearated Learning and Unlearning Training...\n"
     ]
    }
   ],
   "source": [
    "print(60*'=')\n",
    "print(\"Step3. Fedearated Learning and Unlearning Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            criteria = nn.CrossEntropyLoss()\n",
    "            test_loss += criteria(output, target) # sum up batch loss\n",
    "            \n",
    "            pred = torch.argmax(output,axis=1)\n",
    "            test_acc += accuracy_score(pred,target)\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = test_acc/np.ceil(len(test_loader.dataset)/test_loader.batch_size)\n",
    "    print('Test set: Average loss: {:.8f}'.format(test_loss))         \n",
    "    print('Test set: Average acc:  {:.4f}'.format(test_acc))    \n",
    "    return (test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def global_train_once(global_model, client_data_loaders, test_loader, FL_params):\n",
    "    device = torch.device(\"cuda\" if FL_params.use_gpu*FL_params.cuda_state else \"cpu\")\n",
    "    device_cpu = torch.device(\"cpu\")\n",
    "    \n",
    "    client_models = []\n",
    "    client_sgds = []\n",
    "    for tt in range(FL_params.N_client):\n",
    "        client_models.append(copy.deepcopy(global_model))\n",
    "        client_sgds.append(optim.SGD(client_models[tt].parameters(), lr=FL_params.local_lr, momentum=0.9))\n",
    "    \n",
    "    for client_idx in range(FL_params.N_client):\n",
    "        if(((FL_params.if_retrain) and (FL_params.forget_client_idx == client_idx)) or ((FL_params.if_unlearning) and (FL_params.forget_client_idx == client_idx))):\n",
    "            continue\n",
    "\n",
    "        model = client_models[client_idx]\n",
    "        optimizer = client_sgds[client_idx]\n",
    "        \n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        \n",
    "        #local training\n",
    "        for local_epoch in range(FL_params.local_epoch):\n",
    "            for batch_idx, (data, target) in enumerate(client_data_loaders[client_idx]):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                pred = model(data)\n",
    "                criteria = nn.CrossEntropyLoss()\n",
    "                loss = criteria(pred, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            if(FL_params.train_with_test):\n",
    "                print(\"Local Client No. {}, Local Epoch: {}\".format(client_idx, local_epoch))\n",
    "                print(\"Loss {}, Acc{}\".format(test(model, test_loader)))\n",
    "        \n",
    "        model.to(device_cpu)\n",
    "        client_models[client_idx] = model\n",
    "        \n",
    "    if(((FL_params.if_retrain) and (FL_params.forget_client_idx == client_idx))):\n",
    "        client_models.pop(FL_params.forget_client_idx)\n",
    "        return client_models\n",
    "    \n",
    "    elif((FL_params.if_unlearning) and (FL_params.forget_client_idx in range(FL_params.N_client))):\n",
    "        client_models.pop(FL_params.forget_client_idx)\n",
    "        return client_models\n",
    "    \n",
    "    else:\n",
    "        return client_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg(local_models):\n",
    "    # 创建一个新的模型实例，该实例是输入列表中第一个局部模型的深度副本\n",
    "    global_model = copy.deepcopy(local_models[0])\n",
    "    # 获取 global_model 的状态字典，它包含模型的参数\n",
    "    avg_state_dict = global_model.state_dict()\n",
    "    \n",
    "    # 初始化一个空列表，用于存储所有局部模型的状态字典\n",
    "    local_state_dicts = list()\n",
    "    for model in local_models:\n",
    "        # 将每个局部模型的状态字典添加到 local_state_dicts 列表中\n",
    "        local_state_dicts.append(model.state_dict())\n",
    "    \n",
    "    # 遍历 avg_state_dict 的所有层（即模型参数）\n",
    "    for layer in avg_state_dict.keys():\n",
    "        # 将全局模型的每一层参数初始化为0\n",
    "        avg_state_dict[layer] *= 0 \n",
    "        for client_idx in range(len(local_models)):\n",
    "            # 将每个客户端模型的相应层参数加到全局模型的相应层\n",
    "            avg_state_dict[layer] += local_state_dicts[client_idx][layer]\n",
    "        # 对每一层，计算所有客户端模型的该层参数的平均值\n",
    "        avg_state_dict[layer] /= len(local_models)\n",
    "    \n",
    "    # 将计算得到的平均参数加载到 global_model 中\n",
    "    global_model.load_state_dict(avg_state_dict)\n",
    "    # 返回具有平均参数的全局模型\n",
    "    return global_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FL_Train(init_global_model, client_data_loaders, test_loader, FL_params):\n",
    "    if(FL_params.if_retrain == True):\n",
    "        raise ValueError('FL_params.if_retrain should be set to False, if you want to train, not retrain FL model')\n",
    "    \n",
    "    if(FL_params.if_unlearning == True):\n",
    "        raise ValueError('FL_params.if_unlearning should be set to False, if you want to train, not unlearning FL model')\n",
    "    \n",
    "    all_global_models = list()\n",
    "    all_client_models = list()\n",
    "    global_model = init_global_model\n",
    "    \n",
    "    all_global_models.append(copy.deepcopy(global_model))\n",
    "    for epoch in range(FL_params.global_epoch):\n",
    "        client_models = global_train_once(global_model, client_data_loaders, test_loader, FL_params)\n",
    "        \n",
    "        all_client_models += client_models\n",
    "        global_model = fedavg(client_models)\n",
    "        print(\"Global Federated Learning epoch = {}\".format(epoch))\n",
    "\n",
    "        all_global_models.append(copy.deepcopy(global_model))\n",
    "        \n",
    "    return all_global_models, all_client_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlearning_step_once(old_client_models, new_client_models, global_model_before_forget, global_model_after_forget):\n",
    "    # 初始化字典，用于存储旧模型和新模型的参数更新\n",
    "    old_param_update = dict()  # 模型参数： oldCM - oldGM_t\n",
    "    new_param_update = dict()  # 模型参数： newCM - newGM_t\n",
    "    \n",
    "    # 获取遗忘后的全局模型的状态字典：newGM_t\n",
    "    new_global_model_state = global_model_after_forget.state_dict()  \n",
    "    \n",
    "    # 初始化返回模型状态字典，用于计算新的全局模型参数\n",
    "    return_model_state = dict()  # newGM_t + ||oldCM - oldGM_t||*(newCM - newGM_t)/||newCM - newGM_t||\n",
    "    \n",
    "    # 确保旧模型列表和新模型列表的长度相同\n",
    "    assert len(old_client_models) == len(new_client_models)\n",
    "    \n",
    "    # 遍历全局模型（遗忘前）的所有层\n",
    "    for layer in global_model_before_forget.state_dict().keys():\n",
    "        # 初始化旧模型和新模型的参数更新字典，以及返回模型状态字典\n",
    "        old_param_update[layer] = 0*global_model_before_forget.state_dict()[layer]\n",
    "        new_param_update[layer] = 0*global_model_before_forget.state_dict()[layer]\n",
    "        return_model_state[layer] = 0*global_model_before_forget.state_dict()[layer]\n",
    "        \n",
    "        # 遍历所有新模型，累加每层的参数\n",
    "        for tt in range(len(new_client_models)):\n",
    "            old_param_update[layer] += old_client_models[tt].state_dict()[layer]\n",
    "            new_param_update[layer] += new_client_models[tt].state_dict()[layer]\n",
    "        # 计算每层参数的平均值：oldCM 和 newCM\n",
    "        old_param_update[layer] /= (tt+1)  \n",
    "        new_param_update[layer] /= (tt+1)  \n",
    "        \n",
    "        # 计算参数更新：oldCM - oldGM_t 和 newCM - newGM_t\n",
    "        old_param_update[layer] = old_param_update[layer] - global_model_before_forget.state_dict()[layer]\n",
    "        new_param_update[layer] = new_param_update[layer] - global_model_after_forget.state_dict()[layer]\n",
    "        \n",
    "        # 计算步长 ||oldCM - oldGM_t|| 和步方向 (newCM - newGM_t)/||newCM - newGM_t||\n",
    "        step_length = torch.norm(old_param_update[layer])\n",
    "        step_direction = new_param_update[layer]/torch.norm(new_param_update[layer])\n",
    "        \n",
    "        # 根据公式计算新的全局模型参数：newGM_t + ||oldCM - oldGM_t||*(newCM - newGM_t)/||newCM - newGM_t||\n",
    "        return_model_state[layer] = new_global_model_state[layer] + step_length*step_direction\n",
    "    \n",
    "    # 创建一个新的全局模型的深拷贝，以避免修改原始模型\n",
    "    return_global_model = copy.deepcopy(global_model_after_forget)\n",
    "    \n",
    "    # 加载计算得到的新全局模型参数\n",
    "    return_global_model.load_state_dict(return_model_state)\n",
    "    \n",
    "    # 返回新的全局模型\n",
    "    return return_global_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlearning(old_GMs, old_CMs, client_data_loaders, test_loader, FL_params):\n",
    "    # 检查是否设置了遗忘选项，如果没有则抛出错误\n",
    "    if(FL_params.if_unlearning == False):\n",
    "        raise ValueError('FL_params.if_unlearning should be set to True, if you want to unlearning with a certain user')\n",
    "    \n",
    "    # 检查遗忘客户端索引是否在正确的范围内，如果不是则抛出错误\n",
    "    if(not(FL_params.forget_client_idx in range(FL_params.N_client))):\n",
    "        raise ValueError('FL_params.forget_client_idx is note assined correctly, forget_client_idx should in {}'.format(range(FL_params.N_client)))\n",
    "    \n",
    "    # 检查遗忘间隔是否设置正确，如果不是则抛出错误\n",
    "    if(FL_params.unlearn_interval == 0 or FL_params.unlearn_interval >FL_params.global_epoch):\n",
    "        raise ValueError('FL_params.unlearn_interval should not be 0, or larger than the number of FL_params.global_epoch')\n",
    "    \n",
    "    # 创建旧模型的深拷贝，以避免修改原始模型\n",
    "    old_global_models = copy.deepcopy(old_GMs)\n",
    "    old_client_models = copy.deepcopy(old_CMs)\n",
    "    \n",
    "    forget_client = FL_params.forget_client_idx  # 获取遗忘客户端索引\n",
    "    \n",
    "    # 遍历所有全局训练轮次\n",
    "    for tt in range(FL_params.global_epoch):\n",
    "        # 提取当前轮次的所有客户端模型\n",
    "        temp = old_client_models[tt * FL_params.N_client : tt * FL_params.N_client + FL_params.N_client]\n",
    "        # 从列表中删除遗忘客户端的模型\n",
    "        temp.pop(forget_client)\n",
    "        # 将修改后的模型列表追加到旧客户端模型列表中\n",
    "        old_client_models.append(temp)\n",
    "    # 只保留最新的客户端模型列表\n",
    "    old_client_models = old_client_models[-FL_params.global_epoch:]\n",
    "    \n",
    "    # 创建全局和客户端模型的选定索引，以便在遗忘过程中选择模型\n",
    "    GM_intv = np.arange(0,FL_params.global_epoch+1, FL_params.unlearn_interval, dtype=np.int16())\n",
    "    CM_intv  = GM_intv -1\n",
    "    CM_intv = CM_intv[1:]\n",
    "    \n",
    "    # 选择要用于遗忘过程的全局和客户端模型\n",
    "    selected_GMs = [old_global_models[tt] for tt in GM_intv]\n",
    "    selected_CMs = [old_client_models[jj] for jj in CM_intv]\n",
    "    \n",
    "    # 步骤1：从初始模型开始，完成第一轮全局训练的模型叠加\n",
    "    epoch = 0\n",
    "    unlearn_global_models = list()  # 初始化一个空列表，用于存储遗忘过程中的全局模型\n",
    "    unlearn_global_models.append(copy.deepcopy(selected_GMs[0]))  # 将初始全局模型添加到列表中\n",
    "    \n",
    "    # 使用联邦平均算法计算新的全局模型\n",
    "    new_global_model = fedavg(selected_CMs[epoch])\n",
    "    unlearn_global_models.append(copy.deepcopy(new_global_model))\n",
    "    print(\"Federated Unlearning Global Epoch  = {}\".format(epoch))\n",
    "    \n",
    "    # 步骤2：以第一轮全局模型为起点，逐渐纠正模型\n",
    "    # 此步骤中，将第一轮全局训练的全局模型用作新训练的起点，并通过保留用户的数据进行少量训练，以获取每个保留用户的局部模型参数的迭代方向。\n",
    "    # 然后，使用标准FL训练中保存的旧客户端模型和旧全局模型，以及遗忘用户时获得的新客户端模型和新全局模型，构建下一轮的全局模型。\n",
    "    # 保存原始的局部和全局训练轮次设置\n",
    "    CONST_local_epoch = copy.deepcopy(FL_params.local_epoch)\n",
    "    FL_params.local_epoch = np.ceil(FL_params.local_epoch*FL_params.forget_local_epoch_ratio)\n",
    "    FL_params.local_epoch = np.int16(FL_params.local_epoch)\n",
    "    \n",
    "    CONST_global_epoch = copy.deepcopy(FL_params.global_epoch)\n",
    "    FL_params.global_epoch = CM_intv.shape[0]\n",
    "    \n",
    "    print('Local Calibration Training epoch = {}'.format(FL_params.local_epoch))\n",
    "    for epoch in range(FL_params.global_epoch):\n",
    "        if(epoch == 0):\n",
    "            continue  # 跳过第0轮，因为已经在步骤1中完成了\n",
    "        print(\"Federated Unlearning Global Epoch  = {}\".format(epoch))\n",
    "        global_model = unlearn_global_models[epoch]  # 获取当前轮次的全局模型\n",
    "        \n",
    "        # 对每个客户端执行一次全局训练\n",
    "        new_client_models  = global_train_once(global_model, client_data_loaders, test_loader, FL_params)\n",
    "        \n",
    "        # 执行一次遗忘步骤，以计算新的全局模型\n",
    "        new_GM = unlearning_step_once(selected_CMs[epoch], new_client_models, selected_GMs[epoch+1], global_model)\n",
    "        \n",
    "        # 将新的全局模型添加到列表中\n",
    "        unlearn_global_models.append(new_GM)\n",
    "    \n",
    "    # 恢复原始的局部和全局训练轮次设置\n",
    "    FL_params.local_epoch = CONST_local_epoch\n",
    "    FL_params.global_epoch = CONST_global_epoch\n",
    "    \n",
    "    # 返回遗忘过程中的全局模型列表\n",
    "    return unlearn_global_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unlearning_without_cali(old_global_models, old_client_models, FL_params):\n",
    "    if(FL_params.if_unlearning == False):\n",
    "        raise ValueError('FL_params.if_unlearning should be set to True, if you want to unlearning with a certain user')\n",
    "        \n",
    "    if(not(FL_params.forget_client_idx in range(FL_params.N_client))):\n",
    "        raise ValueError('FL_params.forget_client_idx is note assined correctly, forget_client_idx should in {}'.format(range(FL_params.N_client)))\n",
    "    \n",
    "    forget_client = FL_params.forget_client_idx\n",
    "    \n",
    "    for tt in range(FL_params.global_epoch):\n",
    "        temp = old_client_models[tt*FL_params.N_client : tt*FL_params.N_client+FL_params.N_client]\n",
    "        temp.pop(forget_client)\n",
    "        old_client_models.append(temp)\n",
    "    old_client_models = old_client_models[-FL_params.global_epoch:]\n",
    "    \n",
    "    uncali_global_models = list()\n",
    "    uncali_global_models.append(copy.deepcopy(old_global_models[0]))\n",
    "    epoch = 0\n",
    "    uncali_global_model = fedavg(old_client_models[epoch])\n",
    "    uncali_global_models.append(copy.deepcopy(uncali_global_model))\n",
    "    print(\"Federated Unlearning without Clibration Global Epoch  = {}\".format(epoch))\n",
    "    \n",
    "    old_param_update = dict()\n",
    "    return_model_state = dict()\n",
    "    \n",
    "    for epoch in range(FL_params.global_epoch):\n",
    "        if(epoch == 0):\n",
    "            continue\n",
    "        print(\"Federated Unlearning Global Epoch  = {}\".format(epoch))\n",
    "        \n",
    "        current_global_model = uncali_global_models[epoch]\n",
    "        current_client_models = old_client_models[epoch]\n",
    "        old_global_model = old_global_models[epoch]  \n",
    "        \n",
    "        for layer in current_global_model.state_dict().keys():\n",
    "            old_param_update[layer] = 0*current_global_model.state_dict()[layer]\n",
    "            return_model_state[layer] = 0*current_global_model.state_dict()[layer]\n",
    "            \n",
    "            for tt in range(len(current_client_models)):\n",
    "                old_param_update[layer] += current_client_models[tt].state_dict()[layer]\n",
    "            old_param_update[layer] /= (tt+1)\n",
    "            \n",
    "            old_param_update[layer] = old_param_update[layer] - old_global_model.state_dict()[layer]\n",
    "\n",
    "            return_model_state[layer] = current_global_model.state_dict()[layer] + old_param_update[layer]\n",
    "            \n",
    "        return_global_model = copy.deepcopy(old_global_models[0])\n",
    "        return_global_model.load_state_dict(return_model_state)\n",
    "            \n",
    "        uncali_global_models.append(return_global_model)\n",
    "\n",
    "    return uncali_global_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_learning_unlearning(init_global_model, client_loaders, test_loader, FL_params):\n",
    "    \"\"\"\n",
    "    初始化全局模型， \n",
    "    用户端数据集\n",
    "    测试集\n",
    "    联邦学习参数\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fed Training\n",
    "    print(5*\"#\" + \"Federated Learning Start\" + 5*\"#\")\n",
    "    std_time = time.time()\n",
    "    old_GMs, old_CMs = FL_Train(init_global_model, client_loaders, test_loader, FL_params)\n",
    "    end_time = time.time()\n",
    "    time_learn = (std_time - end_time)\n",
    "    print(5*\"#\"+\"Federated Learning End\"+5*\"#\")\n",
    "    \n",
    "    print(5*\"#\"+\"Federated Unlearning Start\"+5*\"#\")\n",
    "    std_time = time.time()\n",
    "    FL_params.if_unlearning = True\n",
    "    FL_params.forget_client_idx = 2\n",
    "    unlearn_GMs = unlearning(old_GMs, old_CMs, client_loaders, test_loader, FL_params) # all_global all_client\n",
    "    end_time = time.time()\n",
    "    time_unlearn = (std_time - end_time)\n",
    "    print(5*\"#\"+\"Federated Unlearning End\"+5*\"#\")\n",
    "    \n",
    "    print(5*\"#\"+\"Federated Unlearning without Calibration Start\"+5*\"#\")\n",
    "    std_time = time.time()\n",
    "    uncali_unlearn_GMs = unlearning_without_cali(old_GMs, old_CMs, FL_params)\n",
    "    end_time = time.time()\n",
    "    time_unlearn_no_cali = (std_time - end_time)\n",
    "    print(5*\"#\"+\"Federated Unlearning without Calibration End\"+5*\"#\")\n",
    "    \n",
    "    print(\"Learning time consuming = {} secods\".format(-time_learn))\n",
    "    print(\"Unlearning time consuming = {} secods\".format(-time_unlearn)) \n",
    "    print(\"Unlearning no Cali time consuming = {} secods\".format(-time_unlearn_no_cali)) \n",
    "    \n",
    "    return old_GMs, unlearn_GMs, uncali_unlearn_GMs, old_CMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Federated Learning Start#####\n",
      "Global Federated Learning epoch = 0\n",
      "Global Federated Learning epoch = 1\n",
      "Global Federated Learning epoch = 2\n",
      "Global Federated Learning epoch = 3\n",
      "Global Federated Learning epoch = 4\n",
      "Global Federated Learning epoch = 5\n",
      "Global Federated Learning epoch = 6\n",
      "Global Federated Learning epoch = 7\n",
      "Global Federated Learning epoch = 8\n",
      "Global Federated Learning epoch = 9\n",
      "Global Federated Learning epoch = 10\n",
      "Global Federated Learning epoch = 11\n",
      "Global Federated Learning epoch = 12\n",
      "Global Federated Learning epoch = 13\n",
      "Global Federated Learning epoch = 14\n",
      "Global Federated Learning epoch = 15\n",
      "Global Federated Learning epoch = 16\n",
      "Global Federated Learning epoch = 17\n",
      "Global Federated Learning epoch = 18\n",
      "Global Federated Learning epoch = 19\n",
      "#####Federated Learning End#####\n",
      "#####Federated Unlearning Start#####\n",
      "Federated Unlearning Global Epoch  = 0\n",
      "Local Calibration Training epoch = 5\n",
      "Federated Unlearning Global Epoch  = 1\n",
      "Federated Unlearning Global Epoch  = 2\n",
      "Federated Unlearning Global Epoch  = 3\n",
      "Federated Unlearning Global Epoch  = 4\n",
      "Federated Unlearning Global Epoch  = 5\n",
      "Federated Unlearning Global Epoch  = 6\n",
      "Federated Unlearning Global Epoch  = 7\n",
      "Federated Unlearning Global Epoch  = 8\n",
      "Federated Unlearning Global Epoch  = 9\n",
      "Federated Unlearning Global Epoch  = 10\n",
      "Federated Unlearning Global Epoch  = 11\n",
      "Federated Unlearning Global Epoch  = 12\n",
      "Federated Unlearning Global Epoch  = 13\n",
      "Federated Unlearning Global Epoch  = 14\n",
      "Federated Unlearning Global Epoch  = 15\n",
      "Federated Unlearning Global Epoch  = 16\n",
      "Federated Unlearning Global Epoch  = 17\n",
      "Federated Unlearning Global Epoch  = 18\n",
      "Federated Unlearning Global Epoch  = 19\n",
      "#####Federated Unlearning End#####\n",
      "#####Federated Unlearning without Calibration Start#####\n",
      "Federated Unlearning without Clibration Global Epoch  = 0\n",
      "Federated Unlearning Global Epoch  = 1\n",
      "Federated Unlearning Global Epoch  = 2\n",
      "Federated Unlearning Global Epoch  = 3\n",
      "Federated Unlearning Global Epoch  = 4\n",
      "Federated Unlearning Global Epoch  = 5\n",
      "Federated Unlearning Global Epoch  = 6\n",
      "Federated Unlearning Global Epoch  = 7\n",
      "Federated Unlearning Global Epoch  = 8\n",
      "Federated Unlearning Global Epoch  = 9\n",
      "Federated Unlearning Global Epoch  = 10\n",
      "Federated Unlearning Global Epoch  = 11\n",
      "Federated Unlearning Global Epoch  = 12\n",
      "Federated Unlearning Global Epoch  = 13\n",
      "Federated Unlearning Global Epoch  = 14\n",
      "Federated Unlearning Global Epoch  = 15\n",
      "Federated Unlearning Global Epoch  = 16\n",
      "Federated Unlearning Global Epoch  = 17\n",
      "Federated Unlearning Global Epoch  = 18\n",
      "Federated Unlearning Global Epoch  = 19\n",
      "#####Federated Unlearning without Calibration End#####\n",
      "Learning time consuming = 78.3844633102417 secods\n",
      "Unlearning time consuming = 30.91362190246582 secods\n",
      "Unlearning no Cali time consuming = 0.045185089111328125 secods\n"
     ]
    }
   ],
   "source": [
    "old_GMs, unlearn_GMs, uncali_unlearn_GMs, _ = \\\n",
    "    federated_learning_unlearning(\n",
    "                                 init_global_model, \n",
    "                                 client_loaders, \n",
    "                                 test_loader, \n",
    "                                 FL_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FL_Retrain(init_global_model, client_data_loaders, test_loader, FL_params):\n",
    "    if(FL_params.if_retrain == False):\n",
    "        raise ValueError('FL_params.if_retrain should be set to True, if you want to retrain FL model')\n",
    "    \n",
    "    if(FL_params.forget_client_idx not in range(FL_params.N_client)):\n",
    "        raise ValueError('FL_params.forget_client_idx should be in [{}], if you want to use standard FL train with forget the certain client dataset.'.format(range(FL_params.N_client)))\n",
    "    \n",
    "    print(5*\"#\"+\"Federated Retraining Start\"+5*\"#\")\n",
    "    print(\"Federated Retrain with Forget Client NO.{}\".format(FL_params.forget_client_idx))\n",
    "    retrain_GMs = list()\n",
    "    all_client_models = list()\n",
    "    retrain_GMs.append(copy.deepcopy(init_global_model))\n",
    "    global_model = init_global_model\n",
    "    for epoch in range(FL_params.global_epoch):\n",
    "        client_models = global_train_once(global_model, client_data_loaders, test_loader, FL_params)\n",
    "        global_model = fedavg(client_models)\n",
    "        print(\"Global Retraining epoch = {}\".format(epoch))\n",
    "        retrain_GMs.append(copy.deepcopy(global_model))\n",
    "        all_client_models += client_models\n",
    "    print(5*\"#\"+\"Federated Retraining End\"+5*\"#\")\n",
    "    \n",
    "    return retrain_GMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "FL_params.if_retrain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####  Federated Retraining Start  #####\n",
      "Federated Retrain with Forget Client NO.2\n",
      "Global Retraining epoch = 0\n",
      "Global Retraining epoch = 1\n",
      "Global Retraining epoch = 2\n",
      "Global Retraining epoch = 3\n",
      "Global Retraining epoch = 4\n",
      "Global Retraining epoch = 5\n",
      "Global Retraining epoch = 6\n",
      "Global Retraining epoch = 7\n",
      "Global Retraining epoch = 8\n",
      "Global Retraining epoch = 9\n",
      "Global Retraining epoch = 10\n",
      "Global Retraining epoch = 11\n",
      "Global Retraining epoch = 12\n",
      "Global Retraining epoch = 13\n",
      "Global Retraining epoch = 14\n",
      "Global Retraining epoch = 15\n",
      "Global Retraining epoch = 16\n",
      "Global Retraining epoch = 17\n",
      "Global Retraining epoch = 18\n",
      "Global Retraining epoch = 19\n",
      "#####Federated Retraining End#####\n",
      "Time using = 66.27519679069519 seconds\n"
     ]
    }
   ],
   "source": [
    "if(FL_params.if_retrain == True):\n",
    "    t1 = time.time()\n",
    "    retrain_GMs = FL_Retrain(init_global_model, client_loaders, test_loader, FL_params)\n",
    "    t2 = time.time()\n",
    "    print(\"Time using = {} seconds\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_attack_model(shadow_old_GM, shadow_client_loaders, shadow_test_loader, FL_params):\n",
    "    shadow_model = shadow_old_GM\n",
    "    n_class_dict = dict()\n",
    "    n_class_dict['adult'] = 2\n",
    "    n_class_dict['purchase'] = 2\n",
    "    n_class_dict['mnist'] = 10\n",
    "    n_class_dict['cifar10'] = 10\n",
    "    \n",
    "    N_class = n_class_dict[FL_params.data_name]\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    shadow_model.to(device)\n",
    "        \n",
    "    shadow_model.eval()\n",
    "\n",
    "    pred_4_mem = torch.zeros([1,N_class])\n",
    "    pred_4_mem = pred_4_mem.to(device)\n",
    "    with torch.no_grad():\n",
    "        for tt in range(len(shadow_client_loaders)):\n",
    "            data_loader = shadow_client_loaders[tt]\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(data_loader):\n",
    "                    data = data.to(device)\n",
    "                    out = shadow_model(data)\n",
    "                    pred_4_mem = torch.cat([pred_4_mem, out])\n",
    "    pred_4_mem = pred_4_mem[1:,:]\n",
    "    pred_4_mem = softmax(pred_4_mem,dim = 1)\n",
    "    pred_4_mem = pred_4_mem.cpu()\n",
    "    pred_4_mem = pred_4_mem.detach().numpy()\n",
    "    \n",
    "    pred_4_nonmem = torch.zeros([1,N_class])\n",
    "    pred_4_nonmem = pred_4_nonmem.to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch, (data, target) in enumerate(shadow_test_loader):\n",
    "            data = data.to(device)\n",
    "            out = shadow_model(data)\n",
    "            pred_4_nonmem = torch.cat([pred_4_nonmem, out])\n",
    "    pred_4_nonmem = pred_4_nonmem[1:,:]\n",
    "    pred_4_nonmem = softmax(pred_4_nonmem,dim = 1)\n",
    "    pred_4_nonmem = pred_4_nonmem.cpu()\n",
    "    pred_4_nonmem = pred_4_nonmem.detach().numpy()\n",
    "    \n",
    "    \n",
    "    #构建MIA 攻击模型 \n",
    "    att_y = np.hstack((np.ones(pred_4_mem.shape[0]), np.zeros(pred_4_nonmem.shape[0])))\n",
    "    att_y = att_y.astype(np.int16)\n",
    "    \n",
    "    att_X = np.vstack((pred_4_mem, pred_4_nonmem))\n",
    "    att_X.sort(axis=1)\n",
    "    \n",
    "    X_train,X_test, y_train, y_test = train_test_split(att_X, att_y, test_size = 0.1)\n",
    "    \n",
    "    attacker = XGBClassifier(\n",
    "                            n_estimators = 300,\n",
    "                            n_jobs = -1,\n",
    "                            max_depth = 30,\n",
    "                            objective = 'binary:logistic',\n",
    "                            booster=\"gbtree\",\n",
    "                            scale_pos_weight = pred_4_nonmem.shape[0]/pred_4_mem.shape[0]\n",
    "                            )\n",
    "    \n",
    "    attacker.fit(X_train, y_train)\n",
    "\n",
    "    return attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(target_model, attack_model, client_loaders, test_loader, FL_params):\n",
    "    n_class_dict = dict()\n",
    "    n_class_dict['adult'] = 2\n",
    "    n_class_dict['purchase'] = 2\n",
    "    n_class_dict['mnist'] = 10\n",
    "    n_class_dict['cifar10'] = 10\n",
    "    \n",
    "    N_class = n_class_dict[FL_params.data_name]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    target_model.to(device)\n",
    "        \n",
    "    target_model.eval()\n",
    "    \n",
    "    unlearn_X = torch.zeros([1,N_class])\n",
    "    unlearn_X = unlearn_X.to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(client_loaders[FL_params.forget_client_idx]):\n",
    "                    data = data.to(device)\n",
    "                    out = target_model(data)\n",
    "                    unlearn_X = torch.cat([unlearn_X, out])\n",
    "                    \n",
    "    unlearn_X = unlearn_X[1:,:]\n",
    "    unlearn_X = softmax(unlearn_X,dim = 1)\n",
    "    unlearn_X = unlearn_X.cpu().detach().numpy()\n",
    "    \n",
    "    unlearn_X.sort(axis=1)\n",
    "    unlearn_y = np.ones(unlearn_X.shape[0])\n",
    "    unlearn_y = unlearn_y.astype(np.int16)\n",
    "    \n",
    "    N_unlearn_sample = len(unlearn_y)\n",
    "    \n",
    "    test_X = torch.zeros([1, N_class])\n",
    "    test_X = test_X.to(device)\n",
    "    with torch.no_grad():\n",
    "        for _, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            out = target_model(data)\n",
    "            test_X = torch.cat([test_X, out])\n",
    "            \n",
    "            if(test_X.shape[0] > N_unlearn_sample):\n",
    "                break\n",
    "    test_X = test_X[1:N_unlearn_sample+1,:]\n",
    "    test_X = softmax(test_X,dim = 1)\n",
    "    test_X = test_X.cpu().detach().numpy()\n",
    "    \n",
    "    test_X.sort(axis=1)\n",
    "    test_y = np.zeros(test_X.shape[0])\n",
    "    test_y = test_y.astype(np.int16)\n",
    "    \n",
    "    XX = np.vstack((unlearn_X, test_X))\n",
    "    YY = np.hstack((unlearn_y, test_y))\n",
    "    \n",
    "    pred_YY = attack_model.predict(XX)\n",
    "    pre = precision_score(YY, pred_YY, pos_label=1)\n",
    "    rec = recall_score(YY, pred_YY, pos_label=1)\n",
    "    print(\"MIA Attacker precision = {:.4f}\".format(pre))\n",
    "    print(\"MIA Attacker recall = {:.4f}\".format(rec))\n",
    "    \n",
    "    return (pre, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Step4. Membership Inference Attack aganist GM...\n",
      "\n",
      "Epoch  = -1\n",
      "Attacking against FL Standard  \n",
      "MIA Attacker precision = 0.9699\n",
      "MIA Attacker recall = 0.9133\n",
      "Attacking against FL Retrain  \n",
      "MIA Attacker precision = 0.6615\n",
      "MIA Attacker recall = 0.4233\n",
      "Attacking against FL Unlearn  \n",
      "MIA Attacker precision = 0.4778\n",
      "MIA Attacker recall = 0.3583\n"
     ]
    }
   ],
   "source": [
    "print(60*'=')\n",
    "print(\"Step4. Membership Inference Attack aganist GM...\")\n",
    "T_epoch = -1\n",
    "old_GM = old_GMs[T_epoch]\n",
    "attack_model = train_attack_model(old_GM, client_loaders, test_loader, FL_params)\n",
    "\n",
    "\n",
    "print(\"\\nEpoch  = {}\".format(T_epoch))\n",
    "print(\"Attacking against FL Standard  \")\n",
    "\n",
    "target_model = old_GMs[T_epoch]\n",
    "(ACC_old, PRE_old) = attack(target_model, attack_model, client_loaders, test_loader, FL_params)\n",
    "\n",
    "if(FL_params.if_retrain == True):\n",
    "    print(\"Attacking against FL Retrain  \")\n",
    "    target_model = retrain_GMs[T_epoch]\n",
    "    (ACC_retrain, PRE_retrain) = attack(target_model, attack_model, client_loaders, test_loader, FL_params)\n",
    "    \n",
    "print(\"Attacking against FL Unlearn  \")\n",
    "target_model = unlearn_GMs[T_epoch]\n",
    "(ACC_unlearn, PRE_unlearn) = attack(target_model, attack_model, client_loaders, test_loader, FL_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adver_attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
